{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "NR1CGv4jFXbi"
   },
   "outputs": [],
   "source": [
    "#Importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import scale\n",
    "%matplotlib inline\n",
    "# Supress Warnings\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "kzB3M7NmFXbl",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# read daṇta\n",
    "churn = pd.read_csv(\"telecom_churn_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V3Z2Tej7FXbo",
    "outputId": "53e51079-2558-40f8-9617-4cbbb1664715"
   },
   "outputs": [],
   "source": [
    "churn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#list of columns\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.DataFrame(churn.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(churn.mobile_number.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zW32f87CFXbt",
    "outputId": "0fda177a-874c-4ee6-bbef-d5d35dd3f940"
   },
   "outputs": [],
   "source": [
    "# feature type summary\n",
    "churn.info(verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T6qXB95oFXbt",
    "outputId": "c419ad9a-0705-4165-fc56-7d47dd9e8b0e"
   },
   "outputs": [],
   "source": [
    "# look at data statistics\n",
    "churn.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YQb5UO7vFXbr",
    "outputId": "237f142c-987a-4cbf-f11d-5cb33c5254c6"
   },
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_rows\", None)\n",
    "(churn.isna().sum()/churn.shape[0])*100\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_rows\", None)\n",
    "(churn.isna().sum()/churn.shape[0])*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Code to convert null values to 0 for all numeric columns\n",
    "all_columns=churn.columns\n",
    "\n",
    "date_columns=[x for x in all_columns if 'date' in x]\n",
    "\n",
    "# Converting dtype of date columns to datetime\n",
    "for col in date_columns:\n",
    "    churn[col] = pd.to_datetime(churn[col], format='%m/%d/%Y')\n",
    "    \n",
    "numeric_columns=[x for x in all_columns if x not in date_columns]\n",
    "churn[numeric_columns]=churn[numeric_columns].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "churn = churn.dropna(thresh=churn.shape[0]*0.6,how='all',axis=1)\n",
    "churn.reset_index(inplace=True,drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(churn.isna().sum()/churn.shape[0])*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### There are few columns more than  75 percent of missing values we can drop those columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### date_of_last_rech_data_6, date_of_last_rech_data_7, date_of_last_rech_data_8 , date_of_last_rech_data_9 removed by using this iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### impute date with medain :last_date_of_month_7,last_date_of_month_8, last_date_of_month_9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###imputing last date of month with actual data\n",
    "from datetime import date,datetime\n",
    "churn['last_date_of_month_7']=churn['last_date_of_month_7'].fillna('7/31/2014')\n",
    "churn['last_date_of_month_8']=churn['last_date_of_month_8'].fillna('8/31/2014')\n",
    "churn['last_date_of_month_9']=churn['last_date_of_month_9'].fillna('9/30/2014')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##imputing with mode\n",
    "churn['date_of_last_rech_6'] = churn['date_of_last_rech_6'].fillna(churn['date_of_last_rech_6'].mode()[0])\n",
    "churn['date_of_last_rech_7'] =churn['date_of_last_rech_7'].fillna(churn['date_of_last_rech_7'].mode()[0])\n",
    "churn['date_of_last_rech_8']=churn['date_of_last_rech_8'].fillna(churn['date_of_last_rech_6'].mode()[0])\n",
    "churn['date_of_last_rech_9']= churn['date_of_last_rech_9'].fillna(churn['date_of_last_rech_9'].mode()[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(churn.isna().sum()/churn.shape[0])*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ncbIazhLFXbs",
    "outputId": "ec852645-5f4d-4ada-b63c-085ee4a2a5aa"
   },
   "source": [
    "### 2) Derive New Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us first extract list of columns containing recharge amount\n",
    "amt_recharge_columns =  churn.columns[churn.columns.str.contains('rech_amt|rech_data')]\n",
    "print(amt_recharge_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vPvmBfKGFXbs"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(churn.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jJ-TUGSGFXbs",
    "outputId": "b30db013-b0b5-4446-abef-a540c2f83ddc"
   },
   "outputs": [],
   "source": [
    "# look at initial rows of the data\n",
    "churn.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "Y4obCivAFXbt"
   },
   "outputs": [],
   "source": [
    "# let's adding new column total recharge amount for data: total_rech_amt_data for calculating High Value customer process\n",
    "churn['total_rech_amt_data_6'] = churn.av_rech_amt_data_6 * churn.total_rech_data_6\n",
    "churn['total_rech_amt_data_7'] = churn.av_rech_amt_data_7 * churn.total_rech_data_7\n",
    "churn['total_rech_amt_data_8'] = churn.av_rech_amt_data_8 * churn.total_rech_data_8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "WWQXhGv2FXbt"
   },
   "outputs": [],
   "source": [
    "churn['total_avg_rech_amnt_6_7_GPhase'] = (churn.total_rech_amt_6 + churn.total_rech_amt_data_6 \\\n",
    "                                               + churn.total_rech_amt_7+ churn.total_rech_amt_data_7)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R7RY4FbXFXbu"
   },
   "outputs": [],
   "source": [
    "# create a filter for values greater than 70th percentile of total average recharge amount for good phase \n",
    "high_value_filter = churn.total_avg_rech_amnt_6_7_GPhase.quantile(0.7)\n",
    "\n",
    "print('70 percentile of 6th and 7th months avg recharge amount: '+str(high_value_filter))\n",
    "\n",
    "telecom_df_high_val_cust = churn[churn.total_avg_rech_amnt_6_7_GPhase > high_value_filter]\n",
    "print('Dataframe Shape after Filtering High Value Customers: ' + str(telecom_df_high_val_cust.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 3) Tag churners and remove attributes of the churn phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "telecom_df_high_val_cust['churn']=telecom_df_high_val_cust['total_ic_mou_9']+telecom_df_high_val_cust['total_og_mou_9']+telecom_df_high_val_cust['vol_2g_mb_9']+telecom_df_high_val_cust['vol_3g_mb_9']\n",
    "telecom_df_high_val_cust['churn']=telecom_df_high_val_cust['churn'].apply(lambda x: 1 if x==0 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us check what's the % of churned customers\n",
    "100*telecom_df_high_val_cust.churn.sum()/len(telecom_df_high_val_cust)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_df_high_val_cust.churn.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### After tagging churners, remove all the attributes corresponding to the churn phase (all attributes having ‘ _9’, etc. in their names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "churn_month_columns =  telecom_df_high_val_cust.columns[telecom_df_high_val_cust.columns.str.contains('_9')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# drop all columns corresponding to the churn phase\n",
    "telecom_df_high_val_cust.drop(churn_month_columns,axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(any(telecom_df_high_val_cust['mobile_number'].duplicated()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Checking for outliers in the continuous variables\n",
    "num_df = telecom_df_high_val_cust[[x for x in telecom_df_high_val_cust.columns if x not in date_columns]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking outliers at 25%,50%,75%,90%,95% and 99%\n",
    "num_df.describe(percentiles=[0.02,.25,.5,.75,.90,.95,.99])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### We can see there are outliers at some places and we can treat those by doing caping of columns. Also we can remove columns like circle_id,log_og_t20_mou , std_og_t2o_mou, loc_ic_t2o_mou , std_ic_t2o_mou_6 , std_ic_t2o_mou_7 , std_ic_t2o_mou_8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "filt_df = telecom_df_high_val_cust[[x for x in telecom_df_high_val_cust.columns if x not in date_columns]]\n",
    "low = .001\n",
    "high = .999\n",
    "quant_df = filt_df.quantile([low, high])\n",
    "quant_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filt_df = filt_df.apply(lambda x: x[(x>=quant_df.loc[low,x.name]) & \n",
    "                                    (x <= quant_df.loc[high,x.name])], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Merging with the leads dataframe\n",
    "telecom_df_high_val_cust = pd.concat([telecom_df_high_val_cust.loc[:, ~telecom_df_high_val_cust.columns.isin([x for x in telecom_df_high_val_cust.columns if x not in date_columns])], filt_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Dropping NA values\n",
    "telecom_df_high_val_cust.dropna(inplace=True)\n",
    "telecom_df_high_val_cust.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_df_high_val_cust.churn.value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us check what's the % of churned customers\n",
    "100*telecom_df_high_val_cust.churn.sum()/len(telecom_df_high_val_cust)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Let's see the correlation matrix \n",
    "\n",
    "telecom_df_high_val_cust.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### We can see here that there are variables that are highly corelated . PCA will take care of this collinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Removing variables that are of no use , since those are not carrying much variance (Few mentioned above in describe like circle_id,log_og_t20_mou , std_og_t2o_mou, loc_ic_t2o_mou , std_ic_t2o_mou_6 , std_ic_t2o_mou_7 , std_ic_t2o_mou_8\n",
    "##along with date columns)\n",
    "\n",
    "telecom_df_high_val_cust=telecom_df_high_val_cust.drop(['circle_id','loc_og_t2o_mou','std_og_t2o_mou','loc_ic_t2o_mou','std_ic_t2o_mou_6','std_ic_t2o_mou_7','std_ic_t2o_mou_8','last_date_of_month_6','last_date_of_month_7','last_date_of_month_8'],1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create box plot for  6th, 7th and 8th month\n",
    "def plot_box_chart(attribute):\n",
    "    plt.figure(figsize=(20,16))\n",
    "    df = telecom_df_high_val_cust\n",
    "    plt.subplot(2,3,1)\n",
    "    sns.boxplot(data=df, y=attribute+\"_6\",x=\"churn\",hue=\"churn\",\n",
    "                showfliers=False,palette=(\"plasma\"))\n",
    "    plt.subplot(2,3,2)\n",
    "    sns.boxplot(data=df, y=attribute+\"_7\",x=\"churn\",hue=\"churn\",\n",
    "                showfliers=False,palette=(\"plasma\"))\n",
    "    plt.subplot(2,3,3)\n",
    "    sns.boxplot(data=df, y=attribute+\"_8\",x=\"churn\",hue=\"churn\",\n",
    "                showfliers=False,palette=(\"plasma\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recharge_amnt_columns =  telecom_df_high_val_cust.columns[telecom_df_high_val_cust.columns.str.contains('rech_amt')]\n",
    "recharge_amnt_columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ploting for total recharge amount:\n",
    "plot_box_chart('total_rech_amt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### We can see a drop in the total recharge amount for churned customers in the 8th Month (Action Phase).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ploting for total recharge amount for data:\n",
    "plot_box_chart('total_rech_amt_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### We can see that there is a huge drop in total recharge amount for data in the 8th month (action phase) for churned customers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ploting for maximum recharge amount for data:\n",
    "plot_box_chart('max_rech_amt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### We can see that there is a huge drop in maximum recharge amount for data in the 8th month (action phase) for churned customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ploting for Total recharge for Number:\n",
    "plot_box_chart('total_rech_num')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### We can see that there is a huge drop in total recharge number also in the 8th month (action phase) for churned customers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### As expected recharge related activities went down in 8th month for churned customers. Similar pattern we may expect in other recrhage related variables too\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 2G and 3G Data related Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "usge_2g_and_3g = telecom_df_high_val_cust.columns[telecom_df_high_val_cust.columns.str.contains('2g|3g',regex=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ploting for volume of 2G and 3G usage columns:\n",
    "plot_box_chart('vol_2g_mb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_box_chart('vol_3g_mb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Similarly here also there is drop in 2G and 3G services also we can see that there proportion of non churn customer are using more internet services so we may say that non availability of better services may contributing to churning of customers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Average Reveneue Per user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking columns for average revenue per user\n",
    "arpu_cols = telecom_df_high_val_cust.columns[telecom_df_high_val_cust.columns.str.contains('arpu_')]\n",
    "\n",
    "# Plotting arpu\n",
    "plot_box_chart('arpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Similar pattern as expected , where we can see drop in arpu in 8th month for churned customer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  Offnet usage "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "offnet_usage_service_col = telecom_df_high_val_cust.columns[telecom_df_high_val_cust.columns.str.contains('offnet.*mou',regex=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Offnet mou values for churned and non churned customers\n",
    "plot_box_chart('offnet_mou')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Customers distribution of the age on network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(telecom_df_high_val_cust.aon.describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Minimun Age on network is 181 days\n",
    "###### Average age on network for customers is 1212 days (3.2 years).\n",
    "###### Around 25% of the HV users are in their 2nd year with the network.\n",
    "###### Around 75% users have Age on network less than 4 years.\n",
    "###### Around 15% users are with the network from over 7 years"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Handling Class Imbalance using SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_df_high_val_cust['churn'].value_counts().plot(kind = 'bar').set_title('churned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Converting datetime to categories\n",
    "telecom_df_high_val_cust['date_of_last_rech_6'] = telecom_df_high_val_cust['date_of_last_rech_6'].astype('category')\n",
    "telecom_df_high_val_cust['date_of_last_rech_7'] = telecom_df_high_val_cust['date_of_last_rech_7'].astype('category')\n",
    "telecom_df_high_val_cust['date_of_last_rech_8'] = telecom_df_high_val_cust['date_of_last_rech_8'].astype('category')\n",
    "\n",
    "cat_columns = telecom_df_high_val_cust.select_dtypes(['category']).columns\n",
    "telecom_df_high_val_cust[cat_columns] = telecom_df_high_val_cust[cat_columns].apply(lambda x: x.cat.codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Separate input features and target\n",
    "y = telecom_df_high_val_cust.churn\n",
    "X = telecom_df_high_val_cust.drop('churn', axis=1)\n",
    "# scaling the features\n",
    "X_scaled = scale(X)\n",
    "# setting up testing and training sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, train_size=0.7, test_size=0.3, random_state=100)\n",
    "\n",
    "sm = SMOTE(random_state=100, sampling_strategy=1.0)\n",
    "X_train, y_train = sm.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts().plot(kind = 'bar').set_title('churned')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Performing PCA for feature reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#Improting the PCA module\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(svd_solver='randomized', random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Doing the PCA on the train data\n",
    "pca.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making the screeplot - plotting the cumulative variance against the number of components\n",
    "%matplotlib inline\n",
    "fig = plt.figure(figsize = (12,8))\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('cumulative explained variance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.cumsum(np.round(pca.explained_variance_ratio_, decimals=4)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  Around 60 components are enough to describe 95% of the variance in the dataset. So We'll choose 60 components for our modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Using incremental PCA for efficiency - saves a lot of time on larger datasets\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "pca_final = IncrementalPCA(n_components=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_pca = pca_final.fit_transform(X_train)\n",
    "df_train_pca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#creating correlation matrix for the principal components\n",
    "corrmat = np.corrcoef(df_train_pca.transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1s -> 0s in diagonals\n",
    "corrmat_nodiag = corrmat - np.diagflat(corrmat.diagonal())\n",
    "print(\"max corr:\",corrmat_nodiag.max(), \", min corr: \", corrmat_nodiag.min(),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying selected components to the test data - 16 components\n",
    "df_test_pca = pca_final.transform(X_test)\n",
    "df_test_pca.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logisitc Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Training the model on the train data\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "\n",
    "learner_pca = LogisticRegression()\n",
    "model_pca = learner_pca.fit(df_train_pca,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_train_pred = model_pca.predict_proba(df_train_pca)[:,1]\n",
    "y_train_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred_final = pd.DataFrame({'Churn':y_train, 'Churn_Prob':y_train_pred})\n",
    "y_train_pred_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create columns with different probability cutoffs \n",
    "numbers = [float(x)/10 for x in range(10)]\n",
    "\n",
    "for i in numbers:\n",
    "    y_train_pred_final[i]= y_train_pred_final.Churn_Prob.map(lambda x: 1 if x > i else 0)\n",
    "y_train_pred_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Now let's calculate accuracy sensitivity and specificity for various probability cutoffs.\n",
    "cutoff_df = pd.DataFrame( columns = ['prob','accuracy','sensi','speci'])\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# TP = confusion[1,1] # true positive \n",
    "# TN = confusion[0,0] # true negatives\n",
    "# FP = confusion[0,1] # false positives\n",
    "# FN = confusion[1,0] # false negatives\n",
    "\n",
    "num = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n",
    "for i in num:\n",
    "    cm1 = metrics.confusion_matrix(y_train_pred_final.Churn, y_train_pred_final[i] )\n",
    "    total1=sum(sum(cm1))\n",
    "    accuracy = (cm1[0,0]+cm1[1,1])/total1\n",
    "    \n",
    "    speci = cm1[0,0]/(cm1[0,0]+cm1[0,1])\n",
    "    sensi = cm1[1,1]/(cm1[1,0]+cm1[1,1])\n",
    "    cutoff_df.loc[i] =[ i ,accuracy,sensi,speci]\n",
    "print(cutoff_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot accuracy sensitivity and specificity for various probabilities.\n",
    "cutoff_df.plot.line(x='prob', y=['accuracy','sensi','speci'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### From the curve above, In between 0.50 and 0.60 is the optimum point to take it as a cutoff probability.Lets take 0.55"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_train_pred_final['final_predicted'] = y_train_pred_final.Churn_Prob.map( lambda x: 1 if x > 0.55 else 0)\n",
    "\n",
    "y_train_pred_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "confusion = metrics.confusion_matrix(y_train_pred_final.Churn, y_train_pred_final.final_predicted )\n",
    "confusion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getModelMetrics(actual_churn=False,pred_churn=False):\n",
    "\n",
    "    confusion = metrics.confusion_matrix(actual_churn, pred_churn)\n",
    "\n",
    "    TP = confusion[1,1] # true positive \n",
    "    TN = confusion[0,0] # true negatives\n",
    "    FP = confusion[0,1] # false positives\n",
    "    FN = confusion[1,0] # false negatives\n",
    "\n",
    "    print(\"Roc_auc_score : {}\".format(metrics.roc_auc_score(actual_churn,pred_churn)))\n",
    "    # Let's see the sensitivity of our logistic regression model\n",
    "    print('Sensitivity/Recall : {}'.format(TP / float(TP+FN)))\n",
    "    # Let us calculate specificity\n",
    "    print('Specificity: {}'.format(TN / float(TN+FP)))\n",
    "    # Calculate false postive rate - predicting churn when customer does not have churned\n",
    "    print('False Positive Rate: {}'.format(FP/ float(TN+FP)))\n",
    "    # positive predictive value \n",
    "    print('Positive predictive value: {}'.format(TP / float(TP+FP)))\n",
    "    # Negative predictive value\n",
    "    print('Negative Predictive value: {}'.format(TN / float(TN+ FN)))\n",
    "    # sklearn precision score value \n",
    "    print('sklearn precision score value: {}'.format(metrics.precision_score(actual_churn, pred_churn )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getModelMetrics(y_train_pred_final.Churn, y_train_pred_final.final_predicted )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#Making prediction on the test data\n",
    "pred_probs_test = model_pca.predict_proba(df_test_pca)[:,1]\n",
    "y_test_df=pd.DataFrame(y_test)\n",
    "y_pred_df=pd.DataFrame(pred_probs_test)\n",
    "y_test_df.reset_index(drop=True, inplace=True)\n",
    "y_pred_df.reset_index(drop=True, inplace=True)\n",
    "y_test_pred_final=pd.concat([y_test_df, y_pred_df],axis=1)\n",
    "#\"{:2.2}\".format(metrics.roc_auc_score(y_test, pred_probs_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Renaming the column \n",
    "y_test_pred_final= y_test_pred_final.rename(columns={ 0 : 'Churn_prob'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Renaming the column \n",
    "y_test_pred_final= y_test_pred_final.rename(columns={ 0 : 'Churn_prob'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#y_test_pred_final.head()\n",
    "y_test_pred_final['final_predicted'] = y_test_pred_final.Churn_prob.map(lambda x: 1 if x > 0.55 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "confusion2 = metrics.confusion_matrix(y_test_pred_final.churn, y_test_pred_final.final_predicted )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getModelMetrics(y_test_pred_final.churn, y_test_pred_final.final_predicted )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = metrics.roc_curve( y_test, pred_probs_test, drop_intermediate = False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def draw_roc( actual, probs ):\n",
    "    fpr, tpr, thresholds = metrics.roc_curve( actual, probs,\n",
    "                                              drop_intermediate = False )\n",
    "    auc_score = metrics.roc_auc_score( actual, probs )\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score )\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic example')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "    return fpr, tpr, thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_roc(y_test, pred_probs_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# using rbf kernel, C=1, default value of gamma\n",
    "\n",
    "model = SVC(C = 1, kernel='rbf')\n",
    "model.fit(df_train_pca,y_train)\n",
    "y_pred = model.predict(df_test_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix\n",
    "confusion_matrix(y_true=y_test, y_pred=y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# accuracy\n",
    "print(\"accuracy\", metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "# precision\n",
    "print(\"precision\", metrics.precision_score(y_test, y_pred))\n",
    "\n",
    "# recall/sensitivity\n",
    "print(\"recall\", metrics.recall_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a KFold object with 5 splits \n",
    "folds = KFold(n_splits = 5, shuffle = True, random_state = 4)\n",
    "\n",
    "# specify range of hyperparameters\n",
    "# Set the parameters by cross-validation\n",
    "hyper_params = [ {'gamma': [1e-2, 1e-3, 1e-4],\n",
    "                     'C': [1, 10, 100, 1000]}]\n",
    "\n",
    "\n",
    "# specify model\n",
    "model = SVC(kernel=\"rbf\")\n",
    "\n",
    "# set up GridSearchCV()\n",
    "model_cv = GridSearchCV(estimator = model, \n",
    "                        param_grid = hyper_params, \n",
    "                        scoring= 'roc_auc', \n",
    "                        cv = folds,\n",
    "                        n_jobs = -1,\n",
    "                        verbose = 1,\n",
    "                        return_train_score=True)      \n",
    "\n",
    "# fit the model\n",
    "model_cv.fit(df_train_pca,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# cv results\n",
    "cv_results = pd.DataFrame(model_cv.cv_results_)\n",
    "cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing the optimal roc score and hyperparameters\n",
    "best_score = model_cv.best_score_\n",
    "best_hyperparams = model_cv.best_params_\n",
    "\n",
    "print(\"The best test score is {0} corresponding to hyperparameters {1}\".format(best_score, best_hyperparams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify optimal hyperparameters\n",
    "best_params = {\"C\": 100, \"gamma\": 0.01, \"kernel\":\"rbf\"}\n",
    "\n",
    "# model\n",
    "model = SVC(C=100, gamma=0.01, kernel=\"rbf\")\n",
    "\n",
    "model.fit(df_train_pca,y_train)\n",
    "y_pred = model.predict(df_test_pca)\n",
    "\n",
    "# metrics\n",
    "print(metrics.confusion_matrix(y_test, y_pred), \"\\n\")\n",
    "print(\"accuracy\", metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"precision\", metrics.precision_score(y_test, y_pred))\n",
    "print(\"sensitivity/recall\", metrics.recall_score(y_test, y_pred))\n",
    "print(\"roc_auc_score\", metrics.roc_auc_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing random forest classifier from sklearn library\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Running the random forest with default parameters.\n",
    "rfc = RandomForestClassifier()\n",
    "# fit\n",
    "rfc.fit(df_train_pca,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Making predictions\n",
    "predictions = rfc.predict(df_test_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Importing classification report and confusion matrix from sklearn metrics\n",
    "from sklearn.metrics import classification_report,confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check the report of our default model\n",
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# GridSearchCV to find optimal min_samples_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# Create the parameter grid based on the results of random search \n",
    "param_grid = {\n",
    "    'max_depth': [4,8,10],\n",
    "    'min_samples_leaf': range(100, 400, 200),\n",
    "    'min_samples_split': range(200, 500, 200),\n",
    "    'n_estimators': [100,200, 300], \n",
    "    'max_features': [3, 6]\n",
    "}\n",
    "# Create a based model\n",
    "rf = RandomForestClassifier()\n",
    "# Instantiate the grid search model\n",
    "grid_search = GridSearchCV(estimator = rf, param_grid = param_grid,scoring= 'roc_auc', \n",
    "                          cv = 3, n_jobs = -1,verbose = 1)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(df_train_pca,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing the optimal accuracy score and hyperparameters\n",
    "print('We can get best score of',grid_search.best_score_,'using',grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# model with the best hyperparameters\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc = RandomForestClassifier(bootstrap=True,\n",
    "                             max_depth=10,\n",
    "                             min_samples_leaf=100, \n",
    "                             min_samples_split=200,\n",
    "                             max_features=6,\n",
    "                             n_estimators=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fit\n",
    "rfc.fit(df_train_pca,y_train)\n",
    "# predict\n",
    "predictions = rfc.predict(df_test_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics\n",
    "print(metrics.confusion_matrix(y_test, predictions), \"\\n\")\n",
    "print(\"accuracy\", metrics.accuracy_score(y_test, predictions))\n",
    "print(\"precision\", metrics.precision_score(y_test, predictions))\n",
    "print(\"sensitivity/recall\", metrics.recall_score(y_test, predictions))\n",
    "print(\"roc_auc_score\", metrics.roc_auc_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion Best Model is : Logisitc regression is best model we can use here . Since for logisitc We have Sensitivity/Recall : 0.7804107424960506\n",
    "## Specificity: 0.8686150265097633 . \n",
    "## For Random forest and SVM we have good overall accuary around 90 but sensitivity is not as good as in logistic model. Here sensitivity(True Positive rate- identification of customer that are going to churn ) is more crucial metrics so we should opt for logisitic based on business case/ explainibility and metrics achieved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "y = telecom_df_high_val_cust.churn\n",
    "X = telecom_df_high_val_cust.drop('churn', axis=1)\n",
    "# scaling the features\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_scaled=scaler.fit_transform(X)\n",
    "X_scaled_df=pd.DataFrame(X_scaled,columns=X.columns)\n",
    "# setting up testing and training sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled_df, y, train_size=0.7, test_size=0.3, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression()\n",
    "from sklearn.feature_selection import RFE\n",
    "rfe = RFE(logreg, 60)             # running RFE with 60 variables as output\n",
    "rfe = rfe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfe.support_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(zip(X_train.columns, rfe.support_, rfe.ranking_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.columns[~rfe.support_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Importing random forest classifier from sklearn library\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Running the random forest with default parameters.\n",
    "rfc = RandomForestClassifier()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Making predictions\n",
    "predictions = rfc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check the report of our default model\n",
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GridSearchCV to find optimal min_samples_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# Create the parameter grid based on the results of random search \n",
    "param_grid = {\n",
    "    'max_depth': [8,12,16],\n",
    "    'min_samples_leaf': range(100, 800, 200),\n",
    "    'min_samples_split': range(200, 1000, 200),\n",
    "    'n_estimators': [100,200, 300], \n",
    "    'max_features': [6,9,12]\n",
    "}\n",
    "# Create a based model\n",
    "rf = RandomForestClassifier()\n",
    "# Instantiate the grid search model\n",
    "grid_search = GridSearchCV(estimator = rf, param_grid = param_grid,scoring= 'roc_auc', \n",
    "                          cv = 3, n_jobs = -1,verbose = 1)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing the optimal accuracy score and hyperparameters\n",
    "print('We can get best score of',grid_search.best_score_,'using',grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model with the best hyperparameters\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc = RandomForestClassifier(bootstrap=True,\n",
    "                             max_depth=12,\n",
    "                             min_samples_leaf=100, \n",
    "                             min_samples_split=200,\n",
    "                             max_features=12,\n",
    "                             n_estimators=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fit\n",
    "rfc.fit(X_train,y_train)\n",
    "# predict\n",
    "predictions = rfc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,40))\n",
    "feat_importances = pd.Series(rfc.feature_importances_, index=X.columns)\n",
    "feat_importances.nlargest(len(X.columns)).sort_values().plot(kind='barh', align='center')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### we can see above are the important factors to predict the churn . Top most 5 features are : total incoming , local incoming, average revenue per unit , maximum recharge , local incoming T2T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Session.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
